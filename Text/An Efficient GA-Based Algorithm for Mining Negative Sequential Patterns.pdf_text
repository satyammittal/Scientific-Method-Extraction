An Efﬁcient GA-Based Algorithm for Mining Negative

Sequential Patterns

Zhigang Zheng1, Yanchang Zhao1,2, Ziye Zuo1, and Longbing Cao1

1 Data Sciences & Knowledge Discovery Research Lab
Centre for Quantum Computation and Intelligent Systems

Faculty of Engineering & IT, University of Technology, Sydney, Australia

{zgzheng,zzuo,lbcao}@it.uts.edu.au

2 Centrelink, Australia

yanchang.zhao@centrelink.gov.au

Abstract. Negative sequential pattern mining has attracted increasing concerns
in recent data mining research because it considers negative relationships between
itemsets, which are ignored by positive sequential pattern mining. However, the
search space for mining negative patterns is much bigger than that for positive
ones. When the support threshold is low, in particular, there will be huge amounts
of negative candidates. This paper proposes a Genetic Algorithm (GA) based al-
gorithm to ﬁnd negative sequential patterns with novel crossover and mutation
operations, which are efﬁcient at passing good genes on to next generations with-
out generating candidates. An effective dynamic ﬁtness function and a pruning
method are also provided to improve performance. The results of extensive ex-
periments show that the proposed method can ﬁnd negative patterns efﬁciently
and has remarkable performance compared with some other algorithms of nega-
tive pattern mining.

Keywords: Negative Sequential Pattern, Genetic Algorithm, Sequence Mining,
Data Mining.

1 Introduction

The concept of discovering sequential patterns was ﬁrstly introduced in 1995 [1], and
aimed at discovering frequent subsequences as patterns in a sequence database, given
a user-speciﬁed minimum support threshold. Some popular algorithms in sequential
pattern mining include AprioriAll [1], Generalized Sequential Patterns (GSP) [10] and
PreﬁxSpan [8]. GSP and AprioriAll are both Apriori-like methods based on breadth-
ﬁrst search, while PreﬁxSpan is based on depth-ﬁrst search. Some other methods, such
as SPADE (Sequential PAttern Discovery using Equivalence classes)[12] and SPAM
(Sequential PAttern Mining)[4], are also widely used in researches.

In contrast to traditional positive sequential patterns, negative sequential patterns
focus on negative relationships between itemsets, in which, absent items are taken into
consideration. We give a simple example to illustrate the difference: suppose p1=<a b c
d> is a positive sequential pattern; p2=<a b ¬c e> is a negative sequential pattern; and
each item, a, b, c, d and e, stands for a claim item code in the customer claim database

M.J. Zaki et al. (Eds.): PAKDD 2010, Part I, LNAI 6118, pp. 262–273, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

An Efﬁcient GA-Based Algorithm for Mining Negative Sequential Patterns

263

of an insurance company. By getting the pattern p1, we can tell that an insurant usually
claims for a, b, c and d in a row. However, only with the pattern p2, we are able to ﬁnd
that given an insurant claim for item a and b, if he/she does NOT claim c, then he/she
would claim item e instead of d. This kind of patterns cannot be described or discovered
by positive sequential pattern mining.

However, in trying to utilize traditional frequent pattern mining algorithms for min-
ing negative patterns, two problems stand in the way. (1) Huge amounts of negative
candidates will be generated by classic breath-ﬁrst search methods. For example, given
10 distinct positive frequent items, there are only 1,000 (=103) 3-item positive can-
didates, but there will be 8,000 (=203) 3-item negative candidates because we should
count 10 negative items in it. (2) Take a 3-item data sequence <a b c>, it can only
support candidates <a>, <b>, <c>, <a b>, <a c>, <b c> and <a b c>. But in the
negative case, data sequence <a b c> not only supports the positive candidates as the
above, but also can match a large bunch of negative candidates, such as <a ¬a>,<b
¬a>,<b ¬b>, <a ¬a c>, <a ¬c c> etc. There are thus still huge amounts of negative
candidates even after effective pruning.

Based on Genetic Algorithm (GA) [5], we propose a new method for mining neg-
ative patterns. GA is an evolvement method, which simulates biological evolution. A
generation pass good genes on to a new generation by crossover and mutation, and the
populations become better and better after many generations. We borrow the ideas of
GA to focus on the space with good genes, because this always ﬁnds more frequent
patterns ﬁrst, resulting in good genes. It is therefore more effective than methods which
treat all candidates equally, especially when a very low support threshold is set. It is
equally possible to ﬁnd long negative patterns at the beginning stage of process.

Our contributions are:

– A GA-based algorithm is proposed to ﬁnd negative sequential patterns efﬁciently. It
obtains new generations by crossover and mutation operations without generating
candidates, and uses dynamic ﬁtness to control population evolution. A pruning
method is also provided to improve performance.

– Extensive experimental results on 3 synthetic datasets and a real-world dataset
show that our algorithm has better performance compared with PNSP[11] and Neg-
GSP[14] especially when the support threshold min sup is very low.

This paper is organized as follows. Section 2 talks about related work. Section 3 brieﬂy
introduces negative sequential patterns and presents formal descriptions of them. Our
GA-based algorithm is then described in Section 4. Section 5 shows experimental re-
sults on some datasets. The paper is concluded in the last section.

2 Related Work

Most research on sequential patterns has focused on positive relationships. In recent
years, some research has started to focus on negative sequential pattern mining.
Zhao et al. [13] proposed a method to ﬁnd negative sequential rules based on SPAM
[4]. However the rules are limited to formats such as <A⇒¬B>, <¬A⇒B>,
<¬A⇒¬B>. Ouyang & Huang [7] extended traditional sequential pattern deﬁnition

Z. Zheng et al.

264
(A,B) to include negative elements such as (¬A,B), (A,¬B) and (¬A,¬B). They put for-
ward an algorithm which ﬁnds both frequent and infrequent sequences and then obtains
negative sequential patterns from infrequent sequences. Nancy et al. [6] designed an al-
gorithm PNSPM and applied the Apriori principle to prune redundant candidates. They
extracted meaningful negative sequences using the interestingness measure; neverthe-
less the above works deﬁned some limited negative sequential patterns, which are not
general enough. Sue-Chen et al. [11] presented more general deﬁnitions of negative se-
quential patterns and proposed an algorithm called PNSP, which extended GSP to deal
with mining negative patterns, but they generated negative candidates in the appending
step, which then may produce a lot of unnecessary candidates.

Some existing researches have used GA for mining the negative association rule and
positive sequential pattern. Bilal and Erhan [2] proposed a method using GA to mine
negative quantitative association rules. They generated uniform initial population, and
used an adaptive mutation probability and an adjusted ﬁtness function. [9] designed a
GA to mine generalized sequential patterns, but it is based on SQL expressions. It is an
instructive work since there are few research works using GA for negative sequential
pattern mining.

3 Problem Statement

3.1 Deﬁnitions
A sequence s is an ordered list of elements, s =<e1 e2 ... en>, where each ei, 1≤i≤n,
is an element. An element ei (1≤i≤k) consists of one or more items. For example,
sequence <a b (c,d) f> consists of 4 elements and (c,d) is an element which includes
two items. The length of a sequence is the number of items in the sequence. A sequence
with k items is called a k-sequence or k-item sequence.

Sequence is a general concept. We extend sequence deﬁnition to positive/negative
sequence. A sequence s=<e1 e2 ... en> is a positive sequence, when each element
ei(1≤i≤n) is a positive element. A sequence s=<e1 e2 ... en> is a negative sequence,
when ∃i, ei(1≤i≤n) is a negative element, which represents the absence of an element.
For example, ¬c and ¬(c,d) are negative elements, so <a b ¬c f> and <a b ¬(c,d) f>
are both negative sequences.
A sequence sr=<er1 er2 ... erm> is a subsequence of another sequence sp=<ep1
ep2 ... epn >, if there exists 1≤i1≤i2≤...≤ik≤pn, er1⊆epi1 , er2⊆epi2 , ..., erk
⊆epik .
A sequence sr is a maximum positive subsequence of another sequence sp, if sr
is a subsequence of sp, and sr includes all positive elements of sp. For example, <a b
f> is maximum positive subsequence of <a b ¬c f> and <a b ¬(c,d) f>.

Deﬁnition 1: Negative Sequential Pattern. If the support value of a negative sequence
is greater than the pre-deﬁned support threshold min sup, and it also meets the following
constraints, then we call it a negative sequential pattern.

1) Items in a single element should be all positive or all negative. The reason is that
a positive item and negative item in the same element are unmeaning. For example, <a
(a,¬b) c> is not allowed since item a and item ¬b are in the same element.

2) Two or more continuous negative elements are not accepted in a negative se-

quence. This constraint is also used by other researchers [11].

An Efﬁcient GA-Based Algorithm for Mining Negative Sequential Patterns

265

3) For each negative item in a negative pattern, its positive item is required to be
frequent. For example, if <¬c> is a negative item, its positive item <c> is required to
be frequent. It is helpful for us to focus on the frequent items.

In order to calculate the support value of a negative sequence against the data se-
quences in a database, we need to clarify the sequence matching method and criteria. In
other words, we should describe what kinds of sequence a data sequence can support.
Deﬁnition 2: Negative Matching. A negative sequence sn=<e1 e2 ... ek> matches

a data sequence s=<d1 d2 ... dm>, iff:

1) s contains the max positive subsequence of sn
2) for each negative element ei(1≤i≤k), there exist integers p, q, r(1≤p≤q≤r≤m)
such that: ∃ei−1⊆dp∧ei+1⊆dr, and for ∀dq, ei(cid:8)⊂dq
For example, see Table 1, sn=<b ¬c a> matches <b d a c>, but does not match <b

d c a>, since the negative element c appears between the element b and a.

Table 1. Pattern matching

√
√

Pattern match Sequence
<b ¬c a>
<b d a>
<b ¬c a>
<b d a c>
<b ¬c a> × <b d c a>

Table 2. Encoding

Sequence

Chromosome

gene1
<a b ¬(c,d)> ⇒ +a

gene2

+b

gene3
¬(c,d)

3.2 Ideas of GA-Based Method

As introduced in Section 1, negative sequential pattern mining may encounter huge
amounts of negative candidates even after effective pruning. It will take a long time to
pass over the dataset many times to get the candidates’ support.

Based on GA, we obtain negative sequential patterns by crossover and mutation,
without generating candidates; high frequent patterns are then selected to be parents to
generate offspring. It will pass the best genes on to the next generations and will always
focus on the space with good genes. By going through many generations, it will obtain
a new and relatively high-quality population.

A key issue is how to ﬁnd all the negative patterns since the GA-based method cannot
ensure locating all of them. We therefore use an incremental population, and add all
negative patterns, which are generated by crossover and mutation during the evolution
process, into population. A dynamic ﬁtness function is proposed to control population
evolution. Ultimately, we can secure almost all the frequent patterns. The proportion
can reach 90% to 100% in our experiments on two synthetic datasets.

4 GA-Based Negative Sequential Pattern Mining Algorithm

The general idea of the algorithm is shown as Fig. 1. We will describe the algorithm
from how to encode a sequence, and then introduce population, selection, crossover,
mutation, pruning, ﬁtness function and so on. A detailed algorithm will then be
introduced.

266

Z. Zheng et al.

Fig. 1. Algorithm Flow

4.1 Encoding

Sequence is mapped into the chromosome code in GA. Both crossover and mutation
operations depend on the chromosome code. We need to deﬁne the chromosome to
represent the problems of negative sequential pattern mining exactly. There are many
different methods to encoding the chromosome, such as binary encoding, permutation
encoding, value encoding and tree encoding [5]. The permutation encoding method
is suitable for ordering problem and its format is consistent with the format of the
sequence data, so we use it for sequence encoding.

Each sequence is mapped into a chromosome. Each element of the sequence is
mapped into a gene in the chromosome, no matter whether the element has one item or
more. Given a sequence <e1 e2 ... en>, it is transformed to a chromosome which has
n-genes. Each gene is composed of a tag and an element. The element includes one or
more items, and the tag indicates that the element is positive or negative. For example,
a negative sequence <a b ¬(c,d)> is mapped into a 3-gene chromosome, see Table 2.

4.2 Population and Selection

In the classical GA method, the number of populations is ﬁxed [5]. We using a ﬁxed
number of populations to produce the next generation, but the populations tended to
contract into one high frequent pattern, and we can only obtain a small part of frequent
patterns. To achieve as many sequential patterns as possible, we potentially needed
a population to cover more individuals. We therefore adjusted the basic GA to suit
negative sequential pattern mining in the following ways.

Initial Population. All 1-item frequent positive patterns are obtained ﬁrst. Based on
the 1-item positive patterns, we transform all of them to their corresponding 1-item
negative sequences, such as transforming the frequent positive sequence <e> to the
negative sequence <¬e>. We then take all positive and negative 1-item patterns as the
initial population.

Population Increase. We do not limit population to a ﬁxed number. When we acquire
new sequential patterns during the evolvement, new patterns are put into the population
for the next selection. If the population has already included the patterns, we ignore
them. To improve the performance of this process, a hash table is used to verify whether
a pattern has already appeared in the population.

An Efﬁcient GA-Based Algorithm for Mining Negative Sequential Patterns

267

Selection. The commonly used selection method is roulette wheel selection [3]. We
have an increased population and the population number depends on the count of se-
quential patterns; thus, we can not use roulette wheel selection because the selection
will be too costly if the population number is huge. We select the top K individuals
with high dynamic ﬁtness (see Section 4.5), where K is a constant number showing
how many individuals will be selected for the next generation. To improve the perfor-
mance of this selection method, we sort all individuals in population in descending order
by dynamic ﬁtness value. In every generation, we only select the ﬁrst K individuals.

4.3 Crossover and Mutation

Crossover. Parents with different lengths are allowed to crossover with each other,
and crossover may happen at different positions to get sequential patterns with varied
lengths. For example, a crossover takes place at a different position, which is shown by
’(cid:10)’ in Table 3. After crossover, it may acquire two children. Child1 <b ¬c e> consists
of the ﬁrst part of parent1 and the second part of parent2. Child2 <d a> consists of
the second part of parent1 and the ﬁrst part of parent2. So we get two children with
different lengths. If a crossover takes place both at the end/head of parent1 and at the
head/end of parent2, as Table 4 shows, child2 will be empty. In that case, we shall set
child2 by reverse. A Crossover Rate is also used to control the probability of cross
over when parents generate their children.

Table 3. Crossover

parent1 b ¬c (cid:4) a ⇒ child1 b ¬c e
d (cid:4) e ⇒ child2 d a
parent2

Table 4. Crossover at head/end
parent1 b ¬c a (cid:4) ⇒ child1 b ¬c a d e
(cid:4) d e ⇒ child2 d e b ¬c a
parent2

Mutation. Mutation is helpful in avoiding contraction of the population to a special
frequent pattern. To introduce mutation into sequence generation, we select a random
position and then replace all genes after that position with 1-item patterns. For example,
given an individual <b ¬c a>, after mutation, it may change to <b d ¬e> if <d> and
<¬e> are 1-item patterns. M utation Rate is a percentage to indicate the probability
of mutation when parents generate their children.

4.4 Pruning

When a new generation is obtained after crossover and mutation, it is necessary to verify
whether the new generation is valid in terms of the constraints for negative sequential
patterns before passing over the whole dataset for their supports.
For a new individual c=<e1 e2 e3 ... en>, c’=<ei ej ... ek> (0<i≤j≤k≤n) is the
max positive subsequence of c, that is to say, ei, ej, ... and ek are all positive elements,
is not frequent, c must be infrequent and
and other elements in c are negative. If c
should be pruned. This method is simple but effective for pruning invalid candidates
without cutting off possible valid individuals by mistake.

(cid:5)

4.5 Fitness Function
In order to evaluate the individuals and decide which are the best for the next generation,
a ﬁtness function for individuals is implemented in GA. We use the ﬁtness function

268

Z. Zheng et al.

shown in Eqn.(1):

ind.f itness = (ind.support − min sup) × DatasetSize.

(1)

Fitness. The ﬁtness function is composed of two parts. Support is the percentage that
indicates how many proportion records are matched by the individual. If support is high,
ﬁtness will be high, so that the individual has good characteristics to pass down to next
generation. min sup is a threshold percentage value for verifying whether a sequence is
frequent. Dataset size is the record count of whole dataset.

Dynamic Fitness. Because the characteristics of the individual have been transmitted
to the next generation by crossover or mutation, the individual should exit after a few
generations. The result will tend to contract to one point if the individual doesn’t exit
gradually. We therefore set a dynamic ﬁtness df itness to every individual in the popu-
lation, shown in Eqn.(2). Its initial value is equal to f itness, but decreases during the
evolvement. It indicates that the individuals in the population will gradually ceased to
evolve. It is like a life value. When an individual’s dynamic ﬁtness is low or close to
0(<0.01), we set it to 0 because we regard it as a wasted individual which cannot be
selected for the next generation.

(cid:2)

ind.df itness =

ind.f itness,
ind.df itness×(1 − DecayRate),

initial set
if ind is selected

(2)

Decay Rate. We set a decay rate to indicate the decrease speed of individual’s ﬁtness.
The decay rate is a percentage value between 0% and 100%. If an individual is selected
by the selection process, its dynamic ﬁtness will decrease by the speed of the decay
rate. If the decay rate is high, dynamic ﬁtness will decrease quickly and individuals will
quickly cease to evolve. Thus, we may get less frequent patterns through a high decay
rate. If we want to obtain the maximum frequent patterns, we can set a low decay rate,
such as 5%, but this will give rise to a longer running time.

4.6 Algorithm Description

Our algorithm is composed of the following six steps.

Step 1:We obtain the initial population which includes all frequent 1-item positive
and 1-item negative sequences. Step 2: Calculate all initial individuals’ ﬁtness. Their
dynamic f itness is set to their f itness. Step 3: We select the top K individuals with
high dynamic ﬁtness from the population. After selection, the dynamic ﬁtness of the
selected individuals is updated by Eqn.(2). Step 4: Crossover and mutation between
the selected individuals to produce the next generation. Step 5: After obtaining the
next generation, we ﬁrst prune invalid individuals and then calculate the frequency and
ﬁtness of remained individuals in new generation. If the frequency of an individual is
greater than min sup, we add it into the population, and set its ﬁtness and dynamic
ﬁtness, but if the population has included this individual, we ignore it. Step 6: Go back
to step 3 and iterate the above process until all individuals in the population are dead
(i.e., their dynamic ﬁtness has become close to 0). The dead individuals are still in

An Efﬁcient GA-Based Algorithm for Mining Negative Sequential Patterns

269

population, but they ceased to evolve. In the end, we obtain the ﬁnal result - whole
population, which is composed of all dead individuals.
The pseudocode of our algorithm is given as follows.

RunGA(min sup, decay rate, crossover rate, mutation rate){

pop = initialPopulation();
for (each individual ind in pop){

ind.f itness = calculateFitness(ind);
ind.df itness = ind.f itness
pop.sum df itness = pop.sum df itness + ind.df itness

}
while ( pop.sum df itness > 0 ){

popK = Selection(pop);
if (Random()<crossover rate) Crossover(popK);
if (Random()<mutation rate) Mutation(popK);
for (each individual ind in popK)

if (Prune(ind) ! =true && ind.sup >= min sup)

pop.add(ind);

}
return pop;

}
Selection(pop){ //Subfunction for selecting top K individuals from population

for (each ind with top K dﬁtness in pop){

popK.add(ind);
ind.df itness = ind.df itness ∗ (1-decay rate);
if (ind.df itness < 0.01) ind.df itness = 0;

}
return popK;

}

5 Experiments

Our algorithm was implemented with Java and tested with three synthetic sequence
datasets generated by an IBM data generator [1] and a real-world dataset. We also im-
plemented the PNSP algorithm [11] and Neg-GSP algorithm [14] with Java for perfor-
mance comparison. All the experiments were conducted on a PC with Intel Core 2 CPU
of 2.9GHz, 2GB memory and Windows XP Professional SP2.

Dataset1(DS1) is C8.T8.S4.I8.DB10k.N1k, which means the average number of
elements in a sequence is 8, the average number of items in an element is 8, the average
length of a maximal pattern consists of 4 elements and each element is composed of 8
items average. The data set contains 10k sequences, the number of items is 1000.

Dataset2(DS2) is C10.T2.5.S4.I2.5.DB100k.N10k.
Dataset3(DS3) is C20.T4.S6.I8.DB10k.N2k.
Dataset4(DS4) is real application data for insurance claims. The data set contains
479 sequences. The average number of elements in a sequence is 30. The minimum
number of elements in a sequence is 1, and the maximum number is 171.

Experiments were done to compare the different Crossover Rate, M utation Rate
and Decay Rate on two synthetic datasets, DS1 and DS2. Each experiment was run
10 times and then the average value was got as the ﬁnal result. We focused on comparing
runtime, the number of patterns and the runtime per pattern, which indicates how long
it takes to get one pattern. The total number of all patterns was determined by PNSP and

270

Z. Zheng et al.

Fig. 2. Different Crossover Rates

Fig. 3. Different Mutation Rates

An Efﬁcient GA-Based Algorithm for Mining Negative Sequential Patterns

271

Fig. 4. Different Decay Rates

Neg-GSP algorithm, and it was then easy to know the proportion of patterns we could
get by using our algorithm. The Y axis (see the 3rd and 4th chart of Fig. 2) indicates
the proportion of patterns.

Crossover Rate. We compared different crossover rates from 60% to 100%. Fig. 2
shows the effect of different crossover rates on DS1 and DS2. With low crossover
rates, such as 60%, we obtained almost the same proportion of patterns as with high
crossover rates (see the 2nd and 5th charts in Fig. 2). The least runtime per pattern is
achieved when the crossover rate is low, so 60% is the best choice for the two datasets
in our experiments.
Mutation Rate. We compared different mutation rates from 0% to 20% on DS1 and
DS2 (see Fig. 3). They show that the mutation rate will not have an outstanding effect,
but if it is set to 0%, it will result in missing a lot of patterns. A Mutation rate of 5-10%
is a good choice because it can produce around 80% patterns for DS1 and above 90%
patterns for DS2. When the mutation rate is 5%, the average runtime per pattern is
lower. We therefore set a mutation rate of 5% for the following experiments.

Decay Rate. Decay rate is a variable that we used to control evolution speed. If the decay
rate is high, individuals will die quickly, so we can get only small proportion of patterns.
If the decay rate is low, we can get more patterns, but a longer runtime is necessary (see
Fig. 4). In order to get all negative sequential patterns, we always choose decay rate=5%,
which enables us to obtain around 90% to 100% patterns on the two datasets.

272

Z. Zheng et al.

Fig. 5. Comparison with PNSP and Neg-GSP Algorithms

Performance Comparison. We compared our algorithm with PNSP and Neg-GSP,
which are two algorithms proposed recently for negative sequential pattern mining. The
tests are based on Crossover Rate=60%, M utation Rate=5% and Decay Rate=5%.
The results (see Fig. 5) on 4 different datasets show that the performance of the GA-
based algorithm is better than PNSP and Neg-GSP when the support threshold is low.
Our algorithm is not better than others when min sup is high, because most patterns
are very short and the GA-based method cannot demonstrate its advantage.

When min sup is high, there are not as many patterns and the patterns are short, so
it is very easy to ﬁnd the patterns with existing methods. However, when min sup is
low, the patterns are longer and the search space is much bigger, so it is time-consuming
to ﬁnd patterns using traditional methods. Using our GA-based algorithm, it is still can
obtain the patterns quickly even though min sup is very low.

An Efﬁcient GA-Based Algorithm for Mining Negative Sequential Patterns

273

6 Conclusions and Future Work

Based on GA, we have proposed a method for negative sequential pattern mining. Ex-
tensive experimental results on synthetic datasets and a real-world dataset show that
the proposed method can ﬁnd negative patterns efﬁciently, and it is better than existing
algorithms when the support threshold min sup is low or when the patterns are long.

In our future work, we will focus on studying new measures including ﬁtness func-
tion, selection and crossover method to make our algorithm more efﬁcient. There should
also be some better methods for pruning. Other work will be to explore post mining to
ﬁnd interesting patterns from the discovered negative sequential patterns. As we have
obtained many negative sequential patterns, the means of ﬁnding interesting and inter-
pretable patterns from them is valuable in industry applications.

References

1. Agrawal, R., Srikant, R.: Mining Sequential Patterns. In: Yu, P.S., Chen, A.L.P. (eds.) 11th
International Conference on Data Engineering, pp. 3–14. IEEE Computer Soc. Press, Taipei
(1995)

2. Bilal, A., Erhan, A.: An Efﬁcient Genetic Algorithm for Automated Mining of Both Positive

and Negative Quantitative Association Rules. Soft. Computing 10(3), 230–237 (2006)

3. Haupt, R.L., Haupt, S.E.: Practical Genetic Algorithms. Wiley, New York (1998)
4. Jay, A., Jason, F., Johannes, G., Tomi, Y.: Sequential PAttern Mining Using a Bitmap Repre-
sentation. In: Proceedings of the Eighth ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, ACM, Edmonton (2002)

5. Mitchell, M.: Introduction to Genetic Algorithms. MIT Press, Cambridge (1996)
6. Nancy, P.L., Hung-Jen, C., Wei-Hua, H.: Mining Negative Sequential Patterns. In: Proceed-
ings of the 6th Conference on WSEAS International Conference on Applied Computer Sci-
ence, vol. 6, WSEAS, Hangzhou (2007)

7. Ouyang, W.M., Huang, Q.H.: Mining Negative Sequential Patterns in Transaction Databases.
In: 2007 International Conference on Machine Learning and Cybernetics, vol. 2, pp. 830–834
(2007)

8. Pei, J., Han, J., Mortazavi-Asl, B., Jianyong, W., Pinto, H., Qiming, C., Dayal, U., Mei-
Chun, H.: Mining Sequential Patterns by Pattern-growth: The PreﬁxSpan Approach. IEEE
Transactions on Knowledge and Data Engineering 16, 1424–1440 (2004)

9. Sandra de Amo, A.d.S.R.J.: Mining Generalized Sequential Patterns Using Genetic Program-

ming. In: ICAI 2003, Las Vegas, Nevada, USA (2003)

10. Srikant, R., Agrawal, R.: Mining Sequential Patterns: Generalizations and Performance Im-
provements. In: Proceedings of the Fifth International Conference on Extending Database
Technology, EDBT (1998)

11. Sue-Chen, H., Ming-Yen, L., Chien-Liang, C.: Mining Negative Sequential Patterns for E-
commerce Recommendations. In: Proceedings of the 2008 IEEE Asia-Paciﬁc Services Com-
puting Conference, IEEE Computer Society Press, Los Alamitos (2008)

12. Zaki, M.J.: SPADE: An Efﬁcient Algorithm for Mining Frequent Sequences. Machine Learn-

ing 42, 31–60 (2001)

13. Zhao, Y., Zhang, H., Cao, L., Zhang, C., Bohlscheid, H.: Efﬁcient Mining of Event-Oriented
Negative Sequential Rules. In: IEEE/WIC/ACM International Conference on Web Intelli-
gence and Intelligent Agent Technology, WI-IAT 2008, vol. 1, pp. 336–342 (2008)

14. Zheng, Z., Zhao, Y., Zuo, Z., Cao, L.: Negative-GSP: An Efﬁcient Method for Mining Neg-
ative Sequential Patterns. In: The 8th Australasian Data Mining Conference. AusDM 2009.
Data Mining and Analytics, Melbourne, Australia, vol. 101, pp. 63–67 (2009)

