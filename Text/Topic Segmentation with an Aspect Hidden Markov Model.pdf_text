Topic Segmentation with an Aspect Hidden Markov Model

David M. Blei

University of California, Berkeley

Dept. of Computer Science

∗

495 Soda Hall

Berkeley, CA, 94720, USA
blei@cs.berkeley.edu

ABSTRACT
We present a novel probabilistic method for topic segmen-
tation on unstructured text. One previous approach to this
problem utilizes the hidden Markov model (HMM) method
for probabilistically modeling sequence data [7]. The HMM
treats a document as mutually independent sets of words
generated by a latent topic variable in a time series. We
extend this idea by embedding Hofmann’s aspect model for
text [5] into the segmenting HMM to form an aspect HMM
(AHMM). In doing so, we provide an intuitive topical depen-
dency between words and a cohesive segmentation model.
We apply this method to segment unbroken streams of New
York Times articles as well as noisy transcripts of radio pro-
grams on SpeechBot 1, an online audio archive indexed by
an automatic speech recognition engine. We provide exper-
imental comparisons which show that the AHMM outper-
forms the HMM for this task.

Keywords
Machine Learning for IR; Topic Detection and Tracking

∗
This work was done during a summer internship at the
COMPAQ Cambridge Research Lab.
1A public web site available at http://www.speechbot.com

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGIR’01, September 9-12, 2001, New Orleans, Louisiana, USA.
Copyright 2001 ACM 1-58113-331-6/01/0009 ...$5.00.

Pedro J. Moreno

Compaq Computer Corporation
Cambridge Research Laboratory

One Cambridge Center

Cambridge, MA, 02142, USA

Pedro.Moreno@compaq.com

1.

INTRODUCTION

In the classical information retrieval (IR) problem, a user
searches in a corpus of text for documents which satisfy
her information needs. This framework assumes a notion of
document i.e. that the corpus is divided into cohesive sets
of words each of which expresses particular information.

In some search-worthy text corpora, such as newswire
feeds, television closed captions, or automatic speech recog-
nition (ASR) transcripts of streaming audio, there is no ex-
plicit representation of a document. There are implicit doc-
ument breaks (e.g. television shows, radio segments) but no
clear demarcations of where they occur. Segmentation is a
critical subtask of the IR problem in these corpora.

To this end, we implemented a novel probabilistic method
of topic segmentation which combines a segmenting hidden
Markov model [7] and an aspect model [5]. In this paper,
we describe our method and demonstrate good results when
applied to noisy ASR transcripts and streams of clean (error-
free) unsegmented text.

This paper is divided into six sections.

In section 2,
we summarize previous techniques and describe how our
method relates to them. In section 3, we describe the stan-
In section 4, we de-
dard HMM segmentation approach.
scribe the theory behind the aspect HMM approach.
In
section 5, we report on experiments on both clean and ASR
text. In section 6, we present our conclusions and sugges-
tions for future work.

2. PREVIOUS WORK

There is a considerable body of previous research on which
this work builds. Hearst [4] developed the TextTiling algo-
rithm which uses a word similarity measure between sen-
tences to ﬁnd the point between paragraphs at which the
topic changes. This approach is eﬀective on clean text with
explicit sentence and paragraph structure. However, it is
diﬃcult to implement on text produced by a speech recog-
nition engine. In addition to the unstructured nature of ASR
output, speech recognition engines on unrestricted audio of-
ten have word error rates in the range of 20% to 50% [6].
Since Hearst’s algorithm computes cosine similarity between
relatively small groups of words on either side of a sentence
boundary, it is unclear whether it is robust in the face of
many erroneous words.

Beeferman et al. [1] introduced a feature-based probabilis-
tic segmentation method which does not require text with

352z

z1

w

w1

L

L

states P (zt+1|zt) is a parameter which is separately tuned
in [7]. We simply use normalized counts of transitions be-
tween clusters in the training set to estimate it. Note that
this model requires a segmented corpus to train, but works
in an unsupervised manner to cluster those segments.

To segment a new document, the stream of text is divided
into a sequence of observations ot of L words each. The
Viterbi algorithm [8], a dynamic programming technique, is
used to ﬁnd the most likely hidden sequence of topic states
Z = {z0, z1, . . . , zT} given an observed sequence of word sets
O = {o0, o1, . . . , oT}. Topic breaks occur when zt (cid:5)= zt+1.

Figure 1: A graphical model representing the seg-
menting HMM. Circles represent random variables
and arrows indicate possibly dependency. The plate
around wt denotes that this random variable is re-
peated L times for each topic variable in the series.

paragraph and sentence structure. Though their method
works well, many of the derived features are based on iden-
tifying cue-words which indicate an impending topic shift.
In our domain, high error rates often cloud such cue words
making them diﬃcult to learn and detect.

The method we present builds directly on the hidden
Markov model (HMM) approach of Mulbregt et al. [7]. We
extend this model by embedding the aspect model [5] in
the HMM. This gives rise to a uniﬁed model within which
we ﬁnd both segment clusters to train transition probabili-
ties and language models to determine observation emission
probabilities.

3. HMM SEGMENTATION

In the segmenting HMM framework, an unsegmented doc-
ument is treated as a collection of mutually independent sets
of words. The model posits that each set is probabilistically
generated by a hidden topic variable in a series. Transition
probabilities between topics determine the value of the next
hidden random variable in the sequence.

The HMM models the following generative process. First,
choose a topic from an initial distribution of topics. Then,
generate a set of L independent words from a distribution
over words associated with that topic. Finally, choose an-
other topic, possibly the same topic, from a distribution of
allowed transitions and repeat this process.

Given a new, unsegmented document, one inverts this pro-
cess by calculating the most likely set of topics which gen-
erated the L-word sets of the given document. Topic breaks
occur at the points where the value of the topic variables
change.
More formally, ot = {wt,1, wt,2, wt,3, . . . , wt,L} are sets of
L words and are generated by a topic zt. Each zt depends
only on zt−1 and the ot are independent of each other given
zt. This is illustrated in the graphical model in ﬁgure 1.

The HMM is parameterized by a transition probability
distribution between topics and a set of topic-based uni-
gram language models P (w|z) for each possible value of z.
To train the model, a set of segments from a corpus is clus-
tered using the k-means algorithm. A unigram language
model is computed for each of these clusters and an appro-
priate smoothing technique is applied to account for spar-
sity. The transition probability distribution between topic

This model is an eﬀective segmentation framework on
both clean and ASR text. However, it suﬀers from the naive
Bayes assumption that the words within each observation
are mutually independent given a topic.
P (wi|z)

P (ot|z) =

L(cid:1)

i=1

As L gets large, this assumption works well for computing
P (ot|z). However, the larger L becomes, the less precise
the resulting segmentation will be since the model can only
hypothesize topic breaks between sets of words. The window
(i.e. L) must be large enough to give an accurate estimate
of P (o|z) while small enough to detect a segmentation point
with good granularity.

4. ASPECT HMM SEGMENTATION

A segmenting aspect HMM (AHMM) is a hidden Markov
model in which each hidden state is an instance of the latent
variable in an embedded aspect model. This aspect model
determines both the observation emission probabilities and
training segment clusters to ﬁnd the transition probabilities.
As in the segmenting HMM, each observation is a set of L
words and we use the Viterbi algorithm to ﬁnd topic breaks.
4.1 The aspect model for documents and words
In this section we summarize Hofmann’s aspect model as

it applies to text. For a detailed discussion, see [5].

The aspect model is a family of probability distributions
over a pair of discrete random variables. In text data, this
pair consists of a document label and a word. It is important
to understand that in the aspect model, a document is not
represented as the set of its words but simply a label which
identiﬁes it.
It is associated with its corresponding set of
words through each document-word pair.

This model posits that the occurrence of a document and
a word are independent of each other given a topic or factor.
Let d denote a segment from a presegmented corpus, w de-
note a word, and z denote a topic. Under this independence
assumption, the joint probability of generating a particular
topic, word, and segment label is

P (d, w, z) = P (d|z)P (w|z)P (z).

The P (w|z) parameter is a language model conditioned on
the hidden factor. The P (d|z) parameter is a probability
distribution over the training segment labels. The P (z) dis-
tribution is a the prior distribution on the hidden factor.

Given a corpus of N segments and the words within those
segments, the training data for an aspect model is the set
n)} for each segment label and each word in
of pairs {(dn, wd

z

z1



w

1

w1

L

L

)

Figure 2: A graphical model representing a segment-
ing AHMM

those segments. We can use the Expectation Maximization
(EM) algorithm [2] to ﬁt the parameters from an uncatego-
rized corpus. This corresponds to learning the underlying
topics of a corpus P (w|z) as well as the degree to which each
training document is about those topics P (d|z).

In the E-step, we compute the posterior probability of the
hidden variable given our current model. In the M-step, we
maximize the log likelihood of the training data with respect
to the parameters P (z), P (d|z), and P (w|z). The E-step is

P (z|d, w) =

(cid:2)

P (z)P (d|z)P (w|z)
)P (w|z(cid:2)
P (z(cid:2)

)P (d|z(cid:2)

z(cid:1)

The M-step is
P (d|z) =

P (w|z) =

P (z) =

(cid:3)

(cid:3)

(cid:3)

(cid:3)

w∈W P (z|d, w)n(d, w)
(cid:3)
d(cid:1)∈D P (z|d(cid:2), w)n(d(cid:2), w)
w∈W
(cid:3)
d∈D P (z|d, w)n(d, w)
(cid:3)
d∈D P (z|d, w(cid:2))n(d, w(cid:2))
(cid:3)
w∈W P (z|d, w)n(d, w)
(cid:3)

w(cid:1)∈W
(cid:3)
d∈D
(cid:3)

z(cid:1)

w∈W

d∈D P (z(cid:2)|d, w(cid:2))n(d, w)

where n(d, w) is the number of times word w appears in
document d.
Since d refers to a training document label, the number
of parameters of P (d|z) grows linearly with the size of the
training data making the aspect model quite prone to over-
ﬁtting. To avoid this, we use tempered EM as described
in [5]. Essentially, we hold out a portion of our training
data for cross validation purposes after the E-step. When
the performance decreases on the hold-out data, we reduce a
parameter β ≤ 1 which tempers the eﬀect of the next M-step
on the parameters of the model. In the case of a segment-
ing AHMM, we cross validate by checking the segmentation
accuracy on a held out set of transcripts (see section 5.3 for
a description of the error measure). We stop training when
reducing β no longer improves performance on the segmen-
tation of the hold-out training data.

4.2 The aspect HMM

The segmenting AHMM is an HMM for which the hid-
den topic state is the z random variable in a trained aspect
model. This is depicted in ﬁgure 2. The AHMM works in
exactly the same way as the HMM except that the words
from the selected hidden factor are generated via the aspect
model rather than independently generated.

To train an AHMM, we train an aspect model on a set
of training segments as described in section 4.1. We cluster
the training segments by the P (d|z) parameter.

cluster(d) = arg max

i

P (d|zi)

Finally, we compute transition probabilities between clusters
and initial probabilities of each cluster.

Note that the aspect model does not actually represent
clusters in the way that we compute them. Each d is repre-
sented by P (d|z), a probability for each latent factor. There
is no theoretical reason that the factor with maximum prob-
ability should indicate a cluster assignment. However, in
practice on our corpora, P (d|z) for a ﬁxed d is peaked to-

wards one value of z. In this case, we feel justiﬁed in assign-
ing each segment to the factor with maximal probability.

The AHMM segments a new document by dividing its
words into observation windows of size L and running the
Viterbi algorithm to ﬁnd the most likely sequence of hidden
topics which generated the given document. Segmentation
breaks occur when the value of the topic variable changes
from one window to the next. The Viterbi algorithm re-
quires the observation probability P (ot|z) for each time step.
While the HMM uses the naive Bayes assumption to com-
pute this distribution, we treat each ot as a new segment
label and compute P (ot|z) via the aspect model.

One problem with the aspect model is that it is not a truly
generative model with respect to document labels. As we
mentioned in section 4.1, the P (d|z) parameter is a discrete
distribution over the set of training documents. Therefore,
the model can only compute conditional probabilities about
those segments which it was exposed to in training. In the
Viterbi algorithm, we need to ﬁnd P (ot|z) for some obser-
vation window ot. This observation is not a document label
that the model has seen before. To properly ﬁnd P (ot|z),
one should retrain the model using EM on the training cor-
pus as well as ot and the words it contains. However, this is
very ineﬃcient. In practice, one can use an online approxi-
mation to EM to ﬁnd P (ot|z). We use a variant as described
in [3].
Let ot,i = {, wt,1, wt,2, . . . , wt,i} where wt,0 =  denotes
no word and ot,L = ot denotes the full observation. We
approximate P (z|ot) recursively as follows.

P (z|ot,0) = P (z)
P (z|ot,i+1) =
1

P (wi+1|z, ot,i)P (z|ot,i)
(cid:3)
z(cid:1) P (wi+1|z(cid:2))P (z(cid:2)|ot,i)

+

P (z|ot,i)

i + 1

i

i + 1

Then we use Bayes rule to ﬁnd P (ot|z).
P (z|ot)P (ot)

P (ot|z) =

P (z)

Note that P (ot) is not a meaningful probability. However,
the Viterbi algorithm only needs to compute P (ot|z) for a
single observation at a time. Thus, P (ot) behaves like a scal-
ing constant and we can compute P (ot|z) up to this factor.
Finally, since the Viterbi algorithm only compares probabil-
ities, we can use this proportional probability without any
loss.

These formulae reﬂect an online approximation of one E-
step in the EM algorithm. We present here an intuitive
derivation to illustrate why they make sense as such an ap-
proximation. We would like to recursively estimate P (z|ot)
from partial estimates of P (z|ot,i). First, notice that ot,0 is
the empty word. This immediately gives us the base case.

P (z|ot,0) = P (z)

We can express P (z|ot,i) in terms of our previous informa-
tion as follows.

P (z|ot,i) =

P (w)P (z|w, ot,i−1)

(cid:2)

w∈ot,i

We assume that, in a partial observation sequence oi, the
marginal probability of selecting any word is simply 1/(i +
1). Observe that when w (cid:5)= wi, the word is assumed to
have been accounted for in P (z|oi−1) and is absorbed in the
conditioning. When w = wi, we can compute P (z|wi, oi−1)
by a simple application of Bayes rule.
P (z|wi, ot,i−1) +
P (wi|z, ot,i−1)P (z|ot,i−1)

P (z|ot,i) =

P (z|ot,i−1)

i + 1

i + 1

1

1

i

=

=

i + 1

i

i + 1

1

i + 1

i

i + 1

P (wi)

+

P (z|ot,i−1)
(cid:3)

P (wi|z)P (z|ot,i−1)
z(cid:1) P (wi|z(cid:2))P (z(cid:2)|ot,i−1)

+

P (z|ot,i−1)

The ﬁnal equation expresses P (z|ot,i) in terms of P (z|ot,i−1).

As the approximator sees more words in a single observation,
it reﬁnes its posterior distribution of the topic. It uses this
reﬁned posterior to weight the distribution of the next word.

5. EXPERIMENTAL RESULTS

We applied this segmentation model to two large corpora.
First, we examined Speechbot transcripts from All Things
Considered (ATC), a daily news program on National Pub-
lic Radio. Our corpus spans 317 shows from August 1998
through December 1999. Within these shows there are 4,917
segments with a vocabulary of 35,777 unique terms. The
shows constitute about 4 million words. We estimated the
word error rate in this corpora to be in the 20% to 50% range
[6]. Note that these are only estimates computed from sam-
pling the corpora as perfect transcripts are unavailable to
us.

Additionally, we analyzed a corpus of 3,830 articles from
the New York Times (NYT) to compare the ASR perfor-
mance with error-free text. This corpus constitutes about 4
million words with a vocabulary of 70,792 unique terms. In
all reported experiments, we learn an aspect model with 20
hidden factors.
5.1 Aspect model EM training

Figure 3 illustrates the performance on held out data dur-
ing the tempered EM training of the aspect model (see sec-
tion 4.1). This ﬁgure shows that the NYT corpus converges
faster than the ATC corpus, despite the larger vocabulary

0.9

0.85

0.8

t

a
a
d

0.75

 
t

u
o
−
d
e
h

l

 
f

 

 

o
P
A
O
C
e
g
a
r
e
v
A

0.7

0.65

0.6

0.55

0.5

0.45

0

10

20

30

40

50

60
Iterations of EM

70

80

90

100

NYT
ATC

Figure 3: Tempered EM convergence in the NYT
(upper line) and ATC (lower line) corpora

size, since the text is error free. Furthermore, the NYT cor-
pus converges to a better success rate (see section 5.3 for
how we measure success).

5.2 Sample results and topic labels

In our experiments, we used three variants of our two cor-
pora. First, we created random sequences of segments from
the ATC corpus. Second, we created random sequences from
the NYT corpus to compare clean versus noisy segmenta-
tion. Finally, we used the actual aired sequences of ATC
segments since this is the domain of the primary problem
which we are trying to tackle.

In the random sequences of segments, we attained almost
perfect segmentation on both corpora. However, the results
are mixed with the original broadcasts of the ATC. Figure 4
shows a segmentation from a correctly sequenced transcript
of ATC on April 29, 1999. The segmentation is not perfect
but hypothesizes the detected topic breaks at approximately
the correct points in the program. At ﬁrst, there seem to be
many missed breaks. We argue however that these missed
story breaks do not always constitute topic breaks and there-
fore are not indicative of the performance of our model. To
illustrate this, we explore a method of topic labeling based
on the language model parameters of the aspect model.
One way of identifying the topics which the segmenter
ﬁnds is by the top ﬁfteen words of the P (w|z) parameter for
the value of z which the Viterbi algorithm assigned to a par-
ticular segment. Figure 5 lists these word sets (denoted by a
letter) as they correspond to the topics in the segmentation
(denoted by a number). For example, story 14 is about the
Israeli/Palestinian conﬂict. Its corresponding segment in the
hypothesis segmentation can be described by the words in
topic F which include peace, israeli, and palestinian.

Analysis of this correspondence often explains missed topic
breaks. Articles 11 and 12 are both about the Kosovar
refugees. Understandably, they are both assigned to topic
A and the break between stories goes undetected.
Note that the segmenter can work even if the top words
of P (w|z) fail to give a good topic description. The story

A

B

A

C

D

E

C

F

A

G

HF

I

J

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

A nato, military, kosovo, said, air, get, today, forces, troops, people, refugees,

says, yugoslav, re, to, war

B president, house, republican, replublicans, clinton, senate, impeachment,

democrats, said, think, get, white, today, people, congress

C school, students, schools, get, know, think, says, people, good, like, two,

just, children, year, education

D get, know, like, good, new, re, just, two, people, time, says, think, music,

see

E says, get, health, people, care, new, two, women, years, re, year, patients,

good, medical, study

F

nato, president, peace, israeli, israel, minister, palestinian, today, said, get,
agreement, prime, kosovo, war, milosevic

G olympic, two, said, new, information, today, good, committee, people, nine-

teen, time, year, internet

H people, get, says, said, think, two, good, new, president, today, time, year,

nineteen, years

Figure 4: A segmentation of All Things Considered
from April 29, 1999. The top diagram is the hypoth-
esis segmentation. The bottom diagram is the true
segmentation.

I

J

get, think, people, know, just, re, says, time, good like, two, don, new,
things, say, see, going

today, said, two, get, president, says, market, economy, good, government,
new, economic, year, percent, time, hundred

about deformed frogs is assigned topic I, a rather generic
language model with no real descriptive words. However,
the subsequent story about the economy ﬁts topic J so well
that the AHMM is able to properly detect the break.
5.3 Quantitative Results

We use the co-occurrence agreement probability (CoAP)
introduced in [1] to quantitatively evaluate our segmenter.
The CoAP is deﬁned as

P (agreement) =

(cid:2)

D(i, j)δR(i, j) ⊕ δH (i, j)

. NPR’s Julie McCarthy reports from NATO headquarters in Brussels
on the status of the air war over Yugoslavia including a missile that
went astray and landed near Sophia the capital of Bulgaria.

. A new NPR Kaiser Kennedy School Poll released today shows sub-

stantial support for current US actions in Yugoslavia.

. Congress is divided in its sentiments about the war in Kosovo.

. Linda updates the news from Littleton Colorado where another fu-
neral was held today and the investigation continues into the planning
of the attack on Columbine High School.

. Linda and Noah read letters from All Things Considered listeners.

. New York City teens react to the Littleton Colorado high school

tragedy.

. Today marks the centennial of the birth of Edward Kennedy Elling-

(i,j)

ton.

The function D(i, j) is a probability distribution over the
distances between words in a document; the δ functions are
1 if the two words fall in the same segment and 0 otherwise;
and ⊕ function indicates agreement between the operands.
In our case, D(i, j) = 1 if the words are k words apart and
0 otherwise. With this choice of D, the CoAP is a measure
of how often a segmentation is correct with respect to two
words that are k words apart in the document. Following
[1], we choose k to be half the average length of a segment
in the training corpus, 170 in the ATC corpus, and 200 in
the NYT corpus.

A useful interpretation of the CoAP is through its com-

pliment [1]
P (disagreement) = P (missed)P (seg) + (1 − P (seg))P (false)
where P (seg) is the a priori probability of a segment, P (missed)
is the probability of missing a segment, and P (false) is the
probability of hypothesizing a segment where there is no
segment.

Figure 6 shows the error and its decomposition for three
experiments: the NYT corpus with randomly generated se-
quences of articles; the ATC corpus with randomly gener-
ated sequences of segments; and the ATC corpus with the
true ordering of segments as they were aired. It is interesting
to note that our system tends to undersegment as indicated
by the high P (missed). Furthermore, in the actual ATC or-

. Government (cid:12)gures indicate teenage pregnancy has fallen sharply re-

ducing the countrys overall birth rate.

	. The Florida legislature is expected Thursday to adopt the nations

(cid:12)rst statewide school voucher program.

 . NPRs Tom Gjelten reports that former Russian Prime Minister Vik-
tor Chernomyrdin has undertaken a twoday diplomatic mission aimed
at restoring peace in Yugoslavia.

. Sarah Chayes reports from Tirana Albania on families that have taken

in Kosovar refugees.

. Barbara Mantel reports on the beginning of e(cid:11)orts to bring some

Kosovar refugees to the U.S temporarily.

. NPRs Mike Shuster reports that a scientist who was (cid:12)red from his job
at the Los Alamos National Laboratory on suspicion that hed trans-
ferred U.S weapons secrets to China may have caused more damage
than previously thought.

. NPR senior news analyst Daniel Schorr says that in the midst of the
crisis in Kosovo the ageold Israeli/Palestinian con(cid:13)ictfor nowstill has
a chance for a peaceful settlement.

. NPRs Wade Goodwyn reports funeral services were held today for
yearold Isaiah Shoels. Shoels was a football player and the only black
student killed in the Columbine High massacre.

. NPRs Richard Harris reports that scientists have discovered why some
North American frogs have been su(cid:11)ering from disturbing deformities
such as extra legs or missing legs.

. NPRs Jim Zarroli reports on Wall Streets prediction that the millen-
nium weekend will pass without signi(cid:12)cant bugs for stock exchanges
or major brokerages.

Figure 5: Summary words (top) and ground truth
summaries (bottom) from the ATC segment in ﬁg-
ure 4

Source

P(missed) P(false) P(disagree)

Random NYT 0.123
Random ATC 0.263
Actual ATC
0.434

0.080
0.052
0.063

0.096
0.143
0.233

Figure 6: CoAP results on the ATC and NYT cor-
pora. In the case of randomly generated transcripts,
the reported results are the mean over ten sets of
random transcripts taken from the same set of test-
ing segments.

AHMM
HMM 

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

 

P
A
o
C
e
g
a
r
e
v
a

0.4

0

20

40

60

80
120
window size (in words)

100

140

160

180

200

Figure 7: Window width vs. CoAP for the HMM
and AHMM in the NYT corpus

derings P (missed) is even higher due to the phenomenon of
multiple segments with similar topics (see section 5.2).

Figure 7 is a comparison between the AHMM and HMM
over window widths from 2 to 200. AHMM segmentation
outperforms HMM segmentation for small window widths.
However, as we increase the window size, the performance of
the aspect model decreases. This is due to two facts. First,
the precision of the segmenter decreases, causing a slight
decrease in score. More importantly however, this behavior
occurs because we are using an approximation of P (ot|z). In
the approximation scheme described in section 4.2, words in
the beginning of the window are weighted more heavily than
words towards the end of the window. Therefore, as the
window size increases, more words make less impact on the
observation distribution and the segmenter does not perform
as well.

The HMM does well on large windows since all words are
counted equally. However, this increase in performance is
at the expense of low segmentation granularity. While the
HMM performs better than the AHMM for large windows,
it never attains the performance of the AHMM in small win-
dows. Typically, the AHMM reaches peak performance at a
window size of 10-15 words. The HMM begins to perform
better than the AHMM at around 100 words.

6. CONCLUSIONS AND FUTURE WORK

In this paper, we have introduced a new approach to text
segmentation using a unique probabilistic model that com-
bines an aspect model with an HMM. This is a uniﬁed
framework within which we learn both document clusters
for training and observation probabilities for new segmenta-
tions. The AHMM does well with small windows of words al-
lowing for a more precise segmentation than with the HMM.
We have experimented with this system on noisy text
sources produced by a speech recognition system. Since our
model does not use syntactic structure information, we can
segment this output and accurately hypothesize topic tran-
sition points. Our results on transcripts produced by the
SpeechBot system are quite encouraging.

Future work in this area has several directions. First, we
would like to incorporate segmentation into the Speechbot
IR framework in a principled way and measure its success.
Second, we would like to use the topic labels to categorize
the corpus of segments and further improve audio browsing
and retrieval. Finally, we would like to explore a temporal
analysis of our data and model long term topic shifts in the
hidden factors and language models.

7. REFERENCES
[1] Doug Beeferman, Adam Berger, and John Laﬀerty.

Statistical models for text segmentation. Machine
Learning, 1999.

[2] A. P. Dempster, N. M. Laird, and D. B. Rubin.

Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
Series B (Methodological), 39(1):1–38, 1977.

[3] Daniel Gildea and Thomas Hofmann. Topic-based

language models using em. EuroSpeech-99, pages
2167–2170, 1999.

[4] Marti A. Hearst. Context and structure in automated

full-text information access. University of California at
Berkeley dissertation. Computer Science Division
Technical Report, 1994.

[5] Thomas Hofmann. Probabilistic latent semantic

indexing. Proceedings of the Twenty-Second Annual
International SIGIR Conference on Research and
Development in Information Retrieval, 1999.

[6] Pedro Moreno, JM Van Thong, Beth Logan, Blair

Fidler, Katrina Maﬀey, and Matthew Moores.
Speechbot: a content-based search index for
multimedia on the web. Proceedings of the ﬁrst IEEE
Paciﬁc-Rim Conference on Multimedia, 2000.

[7] P. van Mulbregt, I. Carp, L. Gillick, S. Lowe, and

J. Yamron. Text segmentation and topic tracking on
broadcast news via a hidden markov model approach.
1998.

[8] Andrew J. Viterbi. Error bounds for convolutional

codes and an asymptotically optimal decoding
algorithm. IEEE Transactions on Information Theory,
13:260–269, 1967.

